import sys
from pathlib import Path
from typing import Iterator, Callable, List
import re
import pprint
from functools import reduce
import click
import csv


def matches(regex, fun: Callable) -> Callable:
    """
    Returns a function that searches for `regex` in some input.
    If there are any matches, `fun` is called on that input.
    """

    def f(input):
        matches = re.search(regex, input)
        if matches is not None:
            return fun(matches)
        else:
            return None

    return f


def dict_to_csv(d):
    for name, value in d.items():
        if isinstance(value, dict):
            # add name to every row generated by the recursive
            # call, and then yield those rows individually
            prev_rows = dict_to_csv(value)
            new_rows = [[name] + row for row in prev_rows]
            for row in new_rows:
                yield row
        else:
            yield [name, value]


class LogFilter:
    def __init__(self, combine = None):
        if isinstance(combine, Callable):
            self.combine = combine
        else:
            self.combine = lambda x: x

    def step(self, log):
        print(len(log))
        return log

    def run(self, log: Iterator[str]):
        res = []
        for datum in self.step(log):
            res.append(datum)

        return self.combine(res)


class Chunker(LogFilter):
    def __init__(self, start, end=None, data: LogFilter = None, combine = None):
        super().__init__(combine)
        self.start = start

        # if we have no end, then have it be the function that
        # always returns None
        if end is None:
            self.end = lambda x: None
        else:
            self.end = end

        # the default log filter is the id filter
        if data is None:
            self.data = LogFilter()
        else:
            self.data = data

    def step(self, log):
        chunk = None
        for line in log:
            # the start filter matches
            if self.start(line) is not None:
                # if we have a chunk, then yield it
                if chunk is not None:
                    yield {
                        chunk[0]: self.data.run(chunk[1])
                    }

                # start a new chunk with this line as head
                chunk = (self.start(line), [])

            # if the end filter matches and we have a chunk
            # add the ending line to the chunk, yield it,
            # and then reset it
            elif self.end(line) is not None and chunk is not None:
                chunk[1].append(self.end(line))
                yield {
                    chunk[0]: self.data.run(chunk[1])
                }
                chunk = None

            # if we are working on a chunk, add the current line
            # to that chunk
            elif chunk is not None:
                chunk[1].append(line)

        # if we reach the end of the log and still have a chunk
        # yield it
        if chunk is not None:
            yield {
                chunk[0]: self.data.run(chunk[1])
            }


class LineFilter(LogFilter):
    def __init__(self, regex, f, combine=None):
        super().__init__(combine)
        self.regex = regex
        self.f = f

    def step(self, log):
        for line in log:
            matches = re.search(self.regex, line)
            if matches is not None:
                yield self.f(matches)


class Combine(LogFilter):
    def __init__(self, *children: List[LogFilter], combine=None):
        super().__init__(combine)
        self.children = children

    def run(self, log):
        res = []
        for child in self.children:
            res.append(child.run(log))

        return self.combine(res)


@click.command()
@click.argument("data_dir")
def process(data_dir):
    data_dir = Path(sys.argv[1])
    stderr_log = data_dir / "results" / "stderr.log"
    log = stderr_log.open("r").readlines()
    log = map(lambda x: x.strip(), log)

    print(f"Reading {data_dir}...", end="")

    egraph_stats = LineFilter(
        r"Size: n=(\d+), e=(\d+)",
        lambda m: {"nodes": int(m.group(1)), "classes": int(m.group(2))}
    )

    search_time = LineFilter(
        r"Search time: (\d+\.\d+)",
        lambda m: {"search": float(m.group(1))}
    )

    apply_time = LineFilter(
        r"Apply time: (\d+\.\d+)",
        lambda m: {"apply": float(m.group(1))}
    )

    # merge a list of dicts into a single dict
    def dict_combine(res):
        if len(res) == 0:
            return res
        else:
            return reduce(lambda a, b: {**a, **b}, res)

    phases_f = Chunker(
        start=matches(r"Starting Phase (\w+)", lambda m: m.group(1)),
        combine=dict_combine,
        data=Combine(
            Chunker(
                start=matches(r"Iteration (\d+)", lambda m: int(m.group(1))),
                combine=dict_combine,
                data=Combine(
                    search_time,
                    egraph_stats,
                    apply_time,
                    combine=lambda x: dict_combine(sum(x, []))
                ),
            ),
            LineFilter(
                r"Initial Program Depth: (\d+)",
                lambda m: {"initial_depth": int(m.group(1))},
                combine=lambda x: x[0]
            ),
            LineFilter(
                r"Final Program Depth: (\d+)",
                lambda m: {"final_depth": int(m.group(1))},
                combine=lambda x: x[0]
            ),
            combine=dict_combine
        ),
    )

    res = list(dict_to_csv(phases_f.run(log)))
    with (data_dir / "data.csv").open("w") as f:
        wr = csv.writer(f)
        wr.writerow(["phase","iteration","name","value"])
        wr.writerows(list(res))

    print("Done")

if __name__ == "__main__":
    process()
