#!/usr/bin/env python3

from pathlib import Path
from typing import Iterator, Callable, List
import re
from functools import reduce
import click
import csv


def matches(regex, fun: Callable) -> Callable:
    """
    Returns a function that searches for `regex` in some input.
    If there are any matches, `fun` is called on that input.
    """

    def f(input):
        matches = re.search(regex, input)
        if matches is not None:
            return fun(matches)
        else:
            return None

    return f


def dict_to_csv(d):
    for name, value in d.items():
        if isinstance(value, dict):
            # add name to every row generated by the recursive
            # call, and then yield those rows individually
            prev_rows = dict_to_csv(value)
            new_rows = [[name] + row for row in prev_rows]
            for row in new_rows:
                yield row
        else:
            yield [name, value]


class LogFilter:
    def __init__(self, combine=None):
        if isinstance(combine, Callable):
            self.combine = combine
        else:
            self.combine = lambda x: x

    def step(self, log):
        print(len(log))
        return log

    def run(self, log: Iterator[str]):
        res = []
        for datum in self.step(log):
            res.append(datum)

        return self.combine(res)


class Chunker(LogFilter):
    """
    Chunks up a log into a series of "chunked" logs, and returns the result
    of another LogFilter applied to these chunks.

    A chunk is defined by a `start` filter, and optionally an `end` filter.
    A chunk starts the first time a line matches `start`. If an `end` filter
    is defined, a chunk ends when the `end` filter matches. If no `end` filter
    is defined, then a chunk ends the next time `start` matches
    (or the end of the log).

    By default, this filter produces a dictionary where the key of each entry
    is whatever the `start` filter returns on a matched line. The value is
    whatever is returned by the `data` LogFilter.

    An optional `combine` parameter can be given that specifies how to combine
    the list of returned dictionaries into a single object. If nothing is
    specified, a list of dictionaries is returned.
    """

    def __init__(self, start, end=None, data: LogFilter = None, combine=None):
        super().__init__(combine)
        self.start = start

        # if we have no end, then have it be the function that
        # always returns None
        if end is None:
            self.end = lambda x: None
        else:
            self.end = end

        # the default log filter is the id filter
        if data is None:
            self.data = LogFilter()
        else:
            self.data = data

    def step(self, log):
        chunk = None
        for line in log:
            # the start filter matches
            if self.start(line) is not None:
                # if we have a chunk, then yield it
                if chunk is not None:
                    yield {
                        chunk[0]: self.data.run(chunk[1])
                    }

                # start a new chunk with this line as head
                chunk = (self.start(line), [])

            # if the end filter matches and we have a chunk
            # add the ending line to the chunk, yield it,
            # and then reset it
            elif self.end(line) is not None and chunk is not None:
                chunk[1].append(self.end(line))
                yield {
                    chunk[0]: self.data.run(chunk[1])
                }
                chunk = None

            # if we are working on a chunk, add the current line
            # to that chunk
            elif chunk is not None:
                chunk[1].append(line)

        # if we reach the end of the log and still have a chunk
        # yield it
        if chunk is not None:
            yield {
                chunk[0]: self.data.run(chunk[1])
            }


class LineFilter(LogFilter):
    def __init__(self, regex, f, combine=None):
        super().__init__(combine)
        self.regex = regex
        self.f = f

    def step(self, log):
        for line in log:
            matches = re.search(self.regex, line)
            if matches is not None:
                yield self.f(matches)


class First(LineFilter):
    def __init__(self, regex, f):
        super().__init__(
            regex,
            f,
            combine=lambda x: x[0] if len(x) > 0 else {}
        )


class Combine(LogFilter):
    """
    Composes a list of LogFilters together into a single LogFilter.
    """

    def __init__(self, *children: List[LogFilter], combine=None):
        super().__init__(combine)
        self.children = children

    def run(self, log):
        res = []
        for child in self.children:
            res.append(child.run(log))

        return self.combine(res)


# merge a list of dicts into a single dict
def dict_combine(res):
    if len(res) == 0:
        return {}
    else:
        return reduce(lambda a, b: {**a, **b}, res)


def unique_dict_append(inp):
    res = {}
    for i, d in enumerate(inp):
        for k, v in d.items():
            res[f"{k}-{i}"] = v
    return res


def pp(f):
    def inner(x):
        print("")
        print("input:", x)
        return f(x)
    return inner


def filter_log(log):
    cost = Chunker(
        start=matches("Runner report", lambda m: "report"),
        # end=matches("Egraph size:", lambda x: x.group(0)),
        end=matches("======================", lambda m: ""),
        combine=lambda x: x[0] if len(x) > 0 else {},
        data=Combine(
            First(
                r"Stop reason: (.*)",
                lambda m: {"stop_reason": m.group(1)}
            ),
            First(
                r"Cost: (\d+.\d+) \(old: .*\)",
                lambda m: {"cost": float(m.group(1))},
            ),
            First(
                r"Iterations: (\d+)",
                lambda m: {"iterations": int(m.group(1))}
            ),
            First(
                r"Egraph size: (\d+) nodes, (\d+) classes",
                lambda m: {
                    "final_nodes": int(m.group(1)),
                    "final_classes": int(m.group(2))
                }
            ),
            combine=dict_combine
        )
    )

    egraph_stats = LineFilter(
        r"Size: n=(\d+), e=(\d+)",
        lambda m: {"nodes": int(m.group(1)), "classes": int(m.group(2))}
    )

    search_time = LineFilter(
        r"Search time: (\d+\.\d+)",
        lambda m: {"search": float(m.group(1))}
    )

    apply_time = LineFilter(
        r"Apply time: (\d+\.\d+)",
        lambda m: {"apply": float(m.group(1))}
    )

    phases_f = Chunker(
        start=matches(r"Starting Phase (\w+)", lambda m: m.group(1)),
        combine=unique_dict_append,
        data=Combine(
            Chunker(
                start=matches(r"Iteration (\d+)", lambda m: int(m.group(1))),
                combine=dict_combine,
                data=Combine(
                    search_time,
                    egraph_stats,
                    apply_time,
                    LineFilter(
                        r"Best cost so far: (\d+\.\d+)",
                        lambda m: {"cost": float(m.group(1))}
                    ),
                    combine=lambda x: dict_combine(sum(x, []))
                ),
            ),
            cost,
            First(
                r"Using (\d+) rules",
                lambda m: {"rules": int(m.group(1))}
            ),
            First(
                r"Initial Program Depth: (\d+)",
                lambda m: {"initial_depth": int(m.group(1))},
            ),
            First(
                r"Final Program Depth: (\d+)",
                lambda m: {"final_depth": int(m.group(1))},
            ),
            combine=dict_combine
        ),
    )

    return phases_f.run(log)


def process_single(data_path):
    stderr_log = data_path / "stderr.log"
    log = stderr_log.open("r").readlines()
    log = map(lambda x: x.strip(), log)

    print(f"Reading {data_path}...", end="")
    data = filter_log(log)

    # pp = pprint.PrettyPrinter(indent=2)
    # print()
    # pp.pprint(data)

    # write out to a csv
    res = list(dict_to_csv(data))
    with (data_path / "data.csv").open("w") as f:
        wr = csv.writer(f)
        wr.writerow(["phase", "iteration", "name", "value"])
        wr.writerows(list(res))

    print("Done")


@click.group()
def cli():
    pass


@cli.command()
@click.argument("data_dir")
def single(data_dir):
    process_single(Path(data_dir))


@cli.command()
@click.argument("parent_dir")
def all(parent_dir):
    parent_dir = Path(parent_dir)

    # find all directories that have a stderr.log
    for log_path in parent_dir.glob("**/stderr.log"):
        # call process single on the containing directory
        # that's what `.parents[1]` gets use.
        process_single(log_path.parents[0])


if __name__ == "__main__":
    cli()
