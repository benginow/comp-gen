#+title: Artifact Evaluation ASPLOS'24 AEC Spring

Website: https://sites.google.com/view/asplos24aec/home
HotCRP: https://asplos24aec-spring.hotcrp.com/

* Building a container with =buildah=
:PROPERTIES:
:header-args:async-shell: :name buildah :results none
:END:

To build, you need =buildah= and a =fedora= machine. To keep the image as small as possible, we start the image from just a base file system and use the host package manager to install packages in the image.

#+begin_src async-shell
buildah unshare ./aec/fedora-build-image.sh
#+end_src

* Installation

** Containerized Installation (recommended for AEC)

The easiest way to run experiments is to use [[https://www.docker.com/][docker]] or [[https://podman.io/][podman]]. I personally prefer =podman= because it's easier to install and get running and has better support for rootless containers. The commands use =podman=, however, you can substitute them for =docker= and they should work exactly the same.

*** Starting the experiment server

You can run experiments by submitting jobs to the experiment server. But first, we need to setup the server. The following command will pull an image from the =ghcr.io= container registry and start the server.

#+begin_src async-shell
mkdir -p completed jobs
podman pull ghcr.io/sgpthomas/isaria-aec:latest
podman run --rm -d \
       -v ./completed:/root/comp-gen/server/completed \
       -v ./jobs:/root/comp-gen/server/jobs \
       -v ./xtensa:/root/xtensa \
       --network slirp4netns:allow_host_loopback=true \
       --name isaria \
       ghcr.io/sgpthomas/isaria-aec
#+end_src

**** Flags explanation

- =--rm= will remove the container after the command executes
- =-it= starts the container in interactive mode and allocates a tty for the process so that we can pass stdin to the server
- =-v host-path:container-path= maps a directory on the host to a directory in the container. We use to pass files back and forth from the container.

** Manual Installation

You can read the =aec/fedora-build-image.sh= script to see a complete set of instructions for fedora. It should be similar for other Linux distributions.

* Generating all the data
:PROPERTIES:
:header-args:async-shell: :name jobs :results none :dir (sgt/dir "server")
:END:

** DONE Overall Performance (Figure 4)
CLOSED: [2023-09-19 Tue 10:19]
:LOGBOOK:
- State "DONE"       from "TODO"       [2023-09-19 Tue 10:19]
:END:

*** Comp-gen Numbers

First generate the jobs.

#+begin_src async-shell
./jobs.py overall_performance
#+end_src

Take a look at what is generated in =server/jobs=

Then copy them to the server. The =--clean= flag removes the local copies of the jobs once they have been uploaded to the server.

#+begin_src async-shell
./sync.py upload --name isaria --dir "~/jobs" --clean
#+end_src

*** Diospyros Numbers

#+begin_src async-shell
./jobs.py diospyros
#+end_src

#+begin_src async-shell
./sync.py upload --name isaria --dir "~/jobs" --clean
#+end_src

*** Estimation

Run the estimation job

#+begin_src async-shell
./jobs.py "estimate:performance"
./sync.py update --name isaria --dir "~/jobs" --clean
#+end_src

*** Download results

#+begin_src async-shell
./sync.py download --name isaria --dir "~/completed"
#+end_src

** DONE Compilation Time (Figure 5)
CLOSED: [2023-09-20 Wed 10:09]
:LOGBOOK:
- State "DONE"       from "TODO"       [2023-09-20 Wed 10:09]
:END:

This uses the overall performance numbers. No new experiments needed.

** DONE Pruning (Figure 6)
CLOSED: [2023-09-19 Tue 10:19]
:LOGBOOK:
- State "DONE"       from "TODO"       [2023-09-19 Tue 10:19]
:END:

This needs the pruning experiments.

#+begin_src async-shell
./jobs.py pruning
#+end_src

Upload the jobs.

#+begin_src async-shell
./sync.py upload --name isaria --dir "~/jobs" --clean
#+end_src

** TODO Ruleset Ablation (Figure 7)

We first need to synthesize rulesets.

The following command will generate the jobs needed for that.

#+begin_src async-shell
./jobs.py ruleset_synthesis
#+end_src

Then we need to compile them with Isaria.

And generate estimation for them. This requires the rulesets existing. If they don't, the job creation can't exist. I would like to be able to start these jobs with the rulesets pre-existing. I probably should put them somewhere else, and then have the person copy them to the right location and name them the right things? Or maybe I don't need them to name them the right things.

*TODO*: run this after running ruleset synthesis

#+begin_src async-shell
./jobs.py ruleset_ablation
#+end_src

#+begin_src async-shell
./jobs.py "estimate:ruleset_ablation"
#+end_src

** TODO New Instructions (Table 2)

This generates the new rulesets.

#+begin_src async-shell
./jobs.py new_instructions_ruleset
#+end_src

And this runs Isaria on them. However, this job seems wack. Because I'm adding rules?? And hardcoding the synthesis path. I should probably change it.

#+begin_src async-shell
./jobs.py test_instruction_ruleset
#+end_src

** TODO Rule Distribution (Figure 8)

This doesn't require any more experiments. We can just grab one of the rule_distribution.csv that we have generated from above. Or maybe we should just generate it from the ruleset directly. I should probably do that.

** DONE Alpha Beta Ablation (Figure 9)
CLOSED: [2023-09-19 Tue 11:29]
:LOGBOOK:
- State "DONE"       from "TODO"       [2023-09-19 Tue 11:29]
:END:

#+begin_src async-shell
./jobs.py alpha_beta_ablation
#+end_src

* Making Figures
* Making a change
