#+title: Artifact Evaluation ASPLOS'24 AEC Spring

Website: https://sites.google.com/view/asplos24aec/home
HotCRP: https://asplos24aec-spring.hotcrp.com/

Here are the instructions to:
1) reproduce the figures in our paper
2) demonstrate the robustness of our artifact by showing how to extend it

Many of our experiments require large amounts of memory to complete, and access to proprietary tools to compile kernels for the Tensilica architecture and perform cycle estimation. Educational licenses are available for free [[https://www.cadence.com/en_US/home/company/cadence-academic-network/university-program.html][here]], but we also separate out the generation of the data from the making of plots. We provide all of the data that we used for the paper.

For each figure, we will split instructions into how to generate the data, and how to make the plot. We will provide the data so that you can generate the plots without waiting for all the experiments to run.

Unless otherwise specified, we assume that all commands are executed from the root of the git repository for =comp-gen=.

* Setup

** Software Prerequisites

You will need the following software to replicate our results.

*** R (for making plots)

We use =ggplot= in =R= to generate the figures in the paper.

**** Mac Installation

I recommend that you use [[https://brew.sh/][brew]] to install R.

=brew install r=

If you don't want to do that for some reason. The R binaries are here: https://cran.r-project.org/bin/macosx/

**** Linux Installation

Refer to your distributions instructions on how to install R. Your distribution probably packages R to make it easy to install.

**** Installing R packages

Once you have R installed, there are a few R packages that we need. Running the following script will download and install the necessary packages.

#+begin_src async-shell
./server/figs/R/install.R
#+end_src

*** Python

We use python for several of the data management scripts. Your OS probably comes with some version of python3. Then, you'll need the following python packages.

#+begin_src async-shell
pip3 install click psutil pandas dfply
#+end_src

*** System dependencies

The =sync.py= script assumes that you have =rsync= installed and available on the path.

We generate our figures with tikz. You'll need =pdflatex= to compile the figures into a pdf.

*** AWSCLI (optional dependency)

This makes it easier to run experiments on EC2 machines. You can name machines and pass their names to various scripts instead of their IP addresses.

Installation instructions are here: https://docs.aws.amazon.com/cli/latest/userguide/getting-started-install.html

Then you will have to connect the CLI to your account:  https://docs.aws.amazon.com/cli/latest/userguide/getting-started-quickstart.html

*** Podman or Docker (for re-running experiments)

You don't need this unless you are setting up the experiment server yourself. 

The experiment server is containerized, so you need a client to run the container. Podman and Docker will both work, and have the same CLI interface. Choose whatever you prefer.

OS specific instructions for installing podman can be found here: https://podman.io/get-started

OS specific Instructions for installing docker can be found here: https://www.docker.com/get-started/

** Starting the experiment server

Ignore this if you are part of the AEC committee. You should have already been provided login access to a running server.

Otherwise, you will need to setup the experiment server. You can run locally on your machine, or remotely on a machine with more resources.

The server is distributed as a container, with all of it's dependencies bundled. This should hopefully make it easy to set up. All you need is a container manager. I prefer =podman=, but =docker= should work the same.

*** Podman

Instructions for installing podman are here: https://podman.io/docs/installation

Then, download the Isaria image.

#+begin_src async-shell
podman pull ghcr.io/sgpthomas/isaria-aec:latest
#+end_src

You will need to make two directories on the host to hold queued jobs and completed jobs.

#+begin_src async-shell
mkdir -p completed jobs
#+end_src

If you don't have an =Xtensa= installation setup, you will not be able to run cycle estimation, but you can still run Isaria. In this case, start the server with:

#+begin_src async-shell
podman run -d \
       -v ./completed:/root/comp-gen/server/completed \
       -v ./jobs:/root/comp-gen/server/jobs \
       --name isaria \
       ghcr.io/sgpthomas/isaria-aec
#+end_src

Otherwise, if you do have an =Xtensa= installation, the following command will start the server with the capability of performing cycle-estimation. The server assumes that you have a =lmgrd= license server running on the host using port =27010=.

#+begin_src async-shell
podman run -d \
       -v ./completed:/root/comp-gen/server/completed \
       -v ./jobs:/root/comp-gen/server/jobs \
       -v ./xtensa:/root/xtensa \
       --network slirp4netns:allow_host_loopback=true \
       --name isaria \
       ghcr.io/sgpthomas/isaria-aec
#+end_src

*** TODO Docker

*WARNING*: untested

Instructions for installing docker are here: https://www.docker.com/get-started/.

Then, download the Isaria image.

#+begin_src async-shell
docker pull ghcr.io/sgpthomas/isaria-aec:latest
#+end_src

You will need to make two directories on the host to hold queued jobs and completed jobs.

#+begin_src async-shell
mkdir -p completed jobs
#+end_src

If you don't have an =Xtensa= installation setup, you will not be able to run cycle estimation, but you can still run Isaria. Start the server with:

#+begin_src async-shell
docker run --rm -d \
       -v ./completed:/root/comp-gen/server/completed \
       -v ./jobs:/root/comp-gen/server/jobs \
       --name isaria \
       ghcr.io/sgpthomas/isaria-aec
#+end_src

Otherwise, if you do have an =Xtensa= installation, the following command will start the server with the capability of performing cycle-estimation.

#+begin_src async-shell
docker run --rm -d \
       -v ./completed:/root/comp-gen/server/completed \
       -v ./jobs:/root/comp-gen/server/jobs \
       -v ./xtensa:/root/xtensa \
       --network slirp4netns:allow_host_loopback=true \
       --name isaria \
       ghcr.io/sgpthomas/isaria-aec
#+end_src

* Reproducing paper results
:PROPERTIES:
:header-args:async-shell: :name aec :results none :dir (sgt/dir "server")
:END:

This section provides a guide on how to generate all of the data needed for the figures presented in the paper, as well as how to generate the plots themselves. For every experiment, the workflow is to
1) generate "jobs" that run the experiments
2) give jobs to the experiment server
3) gather completed jobs and generate figures

We have provided all of the data that we used in the paper, so that you can generate the figures without waiting for all the experiments to finish.

For some jobs, you will also need to run cycle estimation. This requires access to the proprietary Xtensa toolchain. They offer educational licenses for free. [[id:setup_xtensa][Here]] are instructions for setting up the tools. For the purposes of artifact evaluation, we will provide a server with the necessary tools installed.

Most of the larger kernels require large amounts of memory. To fully reproduce our results, you will need a machine with XXX ram.

All commands in this section are relative to =comp-gen/server=.

** Overall performance (Figure 4 & 5)

These figures explore how well the programs that an Isaria compiler generates performs compared against Diospyros, and some other tools. We look at both the estimated cycles of compiled programs as well as how long it took to generate them.

*** Generate data

Time estimate: XXX minutes
Memory requirements: XXX gb

#+begin_src async-shell
# generates jobs that run Isaria on all benchmarks
./jobs.py overall_performance

# generates a job that runs cycle estimation on all benchmarks
./jobs.py "estimate:performance" --after performance
#+end_src

If the experiment server is running locally, =./jobs.py= will put jobs into the correct location be default. Otherwise, you have to copy them to the server yourself.

#+begin_src async-shell
./sync.py upload --ip <ip-of-machine> --clean
#+end_src

Once the experiments have finished (there are no jobs left in the jobs directory), you can copy the data locally again with:

#+begin_src async-shell
./sync.py download --ip <ip-of-machine> --clean
#+end_src

Then, we can collate the data. TODO, maybe add more words here?

#+begin_src async-shell
./query.py update est_cycles -t latest --commit
./query.py update diospyros -t latest --commit
#+end_src

*** Make Plots

Navigate to =server/figs= and then running the following two commands will generate the cycle performance graph and the compile time graph.

#+begin_src async-shell :dir (sgt/dir "server" "figs")
./R/generate.R cycle_performance
./R/generate.R compile_time
#+end_src

These are generated as pdfs in =server/figs=.

** Exploration of the effect of pruning (Figure 6)

*** Generate data

#+begin_src async-shell
./jobs.py pruning
./jobs.py "estimate:pruning" --after pruning
#+end_src

Upload the jobs to the server.

#+begin_src async-shell
./sync.py upload --ip <ip-of-machine> --clean
#+end_src

Once they are finished, you can download them.

#+begin_src async-shell
./sync.py download --ip <ip-of-machine> --clean
#+end_src

Finally, run the pruning query over the returned data.

#+begin_src async-shell
./query.py update pruning -t latest --commit
#+end_src

*** Make Plots

You can generate the pruning figure with the following command:

#+begin_src async-shell :dir (sgt/dir "server" "figs")
cd figs
./R/generate.R pruning
#+end_src

** Exploration of time spent generating rules (Figure 7)

Impact of the timeout for rule generation on performance of kernels compiled by Isaria. Investing more time into rule generation has little impact for small kernels, although larger kernels benefit from finding more vectorization rules.

*** Generate Data

There are two stages to the data generation. We first need to generate rulesets. You can skip this step and just use the provided rulesets if you don't want to spend the time generating new rulesets.

#+begin_src async-shell
./jobs.py ruleset_synthesis
#+end_src

If you want to use the provided rulesets, use the following command to generate some new jobs.

#+begin_src async-shell
./jobs.py ruleset_ablation --rulesets rulesets/ablation
./jobs.py "estimate:ruleset_ablation"
#+end_src

*TODO:* Otherwise, this command will use the generated rulesets.

#+begin_src async-shell
TODO
#+end_src

Generate the estimation jobs for ruleset_ablation

#+begin_src async-shell
./jobs.py "estimate:ruleset_ablation" --after ruleset_ablation
#+end_src

Then you can do the standard thing of uploading the jobs, waiting for them to complete, and then downloading the results.

#+begin_src async-shell
./sync.py upload --ip <ip-of-machine> --clean
./sync.py download --ip <ip-of-machine> --clean
#+end_src

Finally, you can run the query to extract the data that we need from the results.

#+begin_src async-shell
./query.py update ruleset_ablation -t latest --commit
#+end_src

*** Make Plots

** Adding new instructions (Table 2)

*** Generate Data
*** Make Plots

** Exploring the effect of alpha and beta parameters (Figure 8 & 9)

*** Generate Data
*** Make Plots

* Old
** Generating all the data
:PROPERTIES:
:header-args:async-shell: :name jobs :results none :dir (sgt/dir "server")
:END:

*** DONE Overall Performance (Figure 4)
CLOSED: [2023-09-19 Tue 10:19]
:LOGBOOK:
- State "DONE"       from "TODO"       [2023-09-19 Tue 10:19]
:END:

**** Comp-gen Numbers

First generate the jobs.

#+begin_src async-shell
./jobs.py overall_performance
#+end_src

Take a look at what is generated in =server/jobs=

Then copy them to the server. The =--clean= flag removes the local copies of the jobs once they have been uploaded to the server.

#+begin_src async-shell
./sync.py upload --name isaria --dir "~/jobs" --clean
#+end_src

**** Diospyros Numbers

#+begin_src async-shell
./jobs.py diospyros
#+end_src

#+begin_src async-shell
./sync.py upload --name isaria --dir "~/jobs" --clean
#+end_src

**** Estimation

Run the estimation job

#+begin_src async-shell
./jobs.py "estimate:performance"
./sync.py update --name isaria --dir "~/jobs" --clean
#+end_src

**** Download results

#+begin_src async-shell
./sync.py download --name isaria --dir "~/completed"
#+end_src

*** DONE Compilation Time (Figure 5)
CLOSED: [2023-09-20 Wed 10:09]
:LOGBOOK:
- State "DONE"       from "TODO"       [2023-09-20 Wed 10:09]
:END:

This uses the overall performance numbers. No new experiments needed.

*** DONE Pruning (Figure 6)
CLOSED: [2023-09-19 Tue 10:19]
:LOGBOOK:
- State "DONE"       from "TODO"       [2023-09-19 Tue 10:19]
:END:

This needs the pruning experiments.

#+begin_src async-shell
./jobs.py pruning
#+end_src

Upload the jobs.

#+begin_src async-shell
./sync.py upload --name isaria --dir "~/jobs" --clean
#+end_src

*** TODO Ruleset Ablation (Figure 7)

We first need to synthesize rulesets.

The following command will generate the jobs needed for that.

#+begin_src async-shell
./jobs.py ruleset_synthesis
#+end_src

Then we need to compile them with Isaria.

And generate estimation for them. This requires the rulesets existing. If they don't, the job creation can't exist. I would like to be able to start these jobs with the rulesets pre-existing. I probably should put them somewhere else, and then have the person copy them to the right location and name them the right things? Or maybe I don't need them to name them the right things.

*TODO*: run this after running ruleset synthesis

#+begin_src async-shell
./jobs.py ruleset_ablation
#+end_src

#+begin_src async-shell
./jobs.py "estimate:ruleset_ablation"
#+end_src

*** TODO New Instructions (Table 2)

This generates the new rulesets.

#+begin_src async-shell
./jobs.py new_instructions_ruleset
#+end_src

And this runs Isaria on them. However, this job seems wack. Because I'm adding rules?? And hardcoding the synthesis path. I should probably change it.

#+begin_src async-shell
./jobs.py test_instruction_ruleset
#+end_src

*** TODO Rule Distribution (Figure 8)

This doesn't require any more experiments. We can just grab one of the rule_distribution.csv that we have generated from above. Or maybe we should just generate it from the ruleset directly. I should probably do that.

*** DONE Alpha Beta Ablation (Figure 9)
CLOSED: [2023-09-19 Tue 11:29]
:LOGBOOK:
- State "DONE"       from "TODO"       [2023-09-19 Tue 11:29]
:END:

#+begin_src async-shell
./jobs.py alpha_beta_ablation
#+end_src

* TODO Making a change

TODO write this section.

* Nitty-Gritty details

This section is for the brave who want to build the experiment server container (or run the server outside of a container), and setup an =xtensa= installation for performing cycle estimates.

** Building plotting container with =buildah=
:PROPERTIES:
:header-args:async-shell: :name buildah :results none
:END:

#+begin_src async-shell
buildah unshare ./aec/fedora-build-figure-image.sh
#+end_src

** Building experiment server container with =buildah=
:PROPERTIES:
:header-args:async-shell: :name buildah :results none
:END:

To build, you need =buildah= and a relatively up-to-date =fedora= machine. To keep the image as small as possible, we start the image from just a base file system and use the host package manager to install packages in the image. Running the =aec/fedora-build-image.sh= inside of a =buildah unshare= session should do all the hard-work for you.

#+begin_src async-shell
buildah unshare ./aec/fedora-build-image.sh
#+end_src

If you want to build and run the server from scratch, read the =fedora-build-image= script to see what all the dependencies are.

** Setting up =XtensaTools=
:PROPERTIES:
:ID: setup_xtensa
:END:

*** Setup XtensaTools

You first need to download the files. Login to the [[https://xpg.cadence.com/cdns-xpg-web/faces/login.xhtml?exp=true][XPG cadence portal]] and then go to the XPG View tab to select the version that you want to install. We did our testing on =RI-2021.8=.

We don't want to have to go through the Xplorer IDE to compile and simulate kernels. So we just need to download the =tools/Xtensa Tools/Xtensa Tools 14.08 for Linux= file by navigating to it, and then pressing the green download button.

You'll also want to download =refernece-cores/Fusion G3 DSP cores for Linux=.

Finally, navigate to the XPG License Manager, and download the license server software. I'm using =v11.15=

You should now have the following files:

#+begin_example
XtensaTools_RI_2021_8_linux.tgz
XRC_FUSIONG3-linux.xws
licserv_linux_x64_v11_15.tgz
#+end_example

Copy these files into a directory named =xtensa=. Extract all of them. The =.xws= file is a zip archive in disguise. You can use =unzip= to extract it's contents.

#+begin_src async-shell
tar xvf XtensaTools_RI_2021_8_linux.tgz
unzip XRC_FUSIONG3-linux.xws
tar xvf licserv_linux_x64_v11_15.tgz
#+end_src

*** Start License server

The last thing that we need to do, is get a license and start the license server.

**** Find machine host-id

You need the host-id of the machine you want to run the license server on to cut the license keys.

#+begin_src async-shell
./x64_lsb/lmutil lmhostid
#+end_src

This will give you the host-id. If you get this error: =./x64_lsb/lmutil: No such file or directory=, then you have to create a symlink as per the following instructions.

***** Fix dynamically linked binaries

The binaries distributed with the license server expect =/lib64/ld-lsb-x86-64.so.3= to exist. For whatever reason, this doesn't exist on the version of Ubuntu that I used. If you run into this, you can link =/lib64/ld-linux-x86-64.so.2= to =/lib64/ld-lsb-x86-64.so.3= which solves the problem.

#+begin_src async-shell
ln -sf /lib64/ld-linux-x86-64.so.2 /lib64/ld-lsb-x86-64.so.3
#+end_src


**** Cut License keys

In the XPG License Manager web interface, press =Add new host=, give it a name, use =Linux MAC= and =floating server= and then enter the host id that you found in the previous step. Then create the host.

Once the host is created, click manage. Add =( + 1 )= for the =New allocation on this host= for all rows, and then press =Cut=. Save to file, and then copy into the =xtensa= directory.

Open the license file, and make the following edits:

1) Change the line starting with =SERVER= to =SERVER <hostname> <host-id> 27010=, filling in hostname with the hostname of the machine you are running on. The =host-id= should already be correct.
2) Change the line starting with =VENDOR xtensad= so that it points to the =x64_lsb= directory inside of the =xtensa= directory. My VENDOR line is =VENDOR xtensad /home/ubuntu/xtensa/x64_lsb/=.

**** Start server

The server expects =/usr/tmp/.flexlm= to exist. You can create it with the command

#+begin_src async-shell
sudo mkdir -p /usr/tmp/.flexlm
#+end_src

Finally, we are all setup to start the server.

#+begin_src async-shell
./x64_lsb/lmgrd -c <LICENSE_FILE>
#+end_src
